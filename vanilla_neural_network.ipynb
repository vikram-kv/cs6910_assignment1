{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-12 17:44:08.659418: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-12 17:44:09.396697: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/vikram/miniconda3/envs/dlenv/lib/\n",
      "2023-02-12 17:44:09.396786: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/vikram/miniconda3/envs/dlenv/lib/\n",
      "2023-02-12 17:44:09.396795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import activation_functions as AF\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "def get_act_by_name(name : str):\n",
    "    if (name == 'linear'):\n",
    "        return AF.Linear.value, AF.Linear.derivative\n",
    "    elif (name == 'sigmoid'):\n",
    "        return AF.Sigmoid.value, AF.Sigmoid.derivative \n",
    "    elif (name == 'tanh'):\n",
    "        return AF.ReLu.value, AF.ReLu.derivative\n",
    "    elif (name == 'relu'):\n",
    "        return AF.TanH.value, AF.TanH.derivative\n",
    "    else:\n",
    "        raise Exception('NotImplemented'); exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATTERN MODE OF OPERATION - one example at a time\n",
    "# USES ONLY SOFTMAX CURRENTLY\n",
    "class ClassificationNeuralNetwork:\n",
    "\n",
    "    def __init__(self, parameters : dict):\n",
    "        self.no_of_hlayers = parameters['NUMBER_OF_HIDDEN_LAYERS']\n",
    "        self.hlayer_sizes = parameters['HIDDEN_LAYER_SIZES']\n",
    "        \n",
    "        self.hlayer_types = parameters['HIDDEN_LAYER_ACTIVATIONS']\n",
    "        self.outlayer_type = parameters['OUTPUT_LAYER_ACTIVATION']\n",
    "        self.indim = parameters['INPUT_DIMENSION']\n",
    "        self.outdim = parameters['NO_OF_CLASSES']\n",
    "        self.eta = parameters['LEARNING_RATE']\n",
    "        self.init_variables()\n",
    "\n",
    "    # random small value init to ensure weights are in non-sat regions of tanh, sigmoid\n",
    "    def init_variables(self):\n",
    "        self.weights = dict()\n",
    "        self.act_valuesderivs = dict()\n",
    "        self.localgradients = dict()\n",
    "        self.layer_outputs = dict()\n",
    "        self.weight_changes = dict()\n",
    "        self.hlayer_sizes[self.no_of_hlayers] = self.outdim\n",
    "        self.hlayer_types[self.no_of_hlayers] = self.outlayer_type\n",
    "        print(self.hlayer_sizes)\n",
    "        for i in range(self.no_of_hlayers + 1):\n",
    "            psize = self.hlayer_sizes[i-1] if i > 0 else self.indim\n",
    "            self.weights[i] = np.random.randn(self.hlayer_sizes[i], psize + 1)\n",
    "            self.act_valuesderivs[i] = np.zeros(self.hlayer_sizes[i])\n",
    "            self.localgradients[i] = np.zeros(self.hlayer_sizes[i])\n",
    "            self.layer_outputs[i] = np.zeros(self.hlayer_sizes[i])\n",
    "            self.weight_changes[i] = np.zeros(self.hlayer_sizes[i])\n",
    "            \n",
    "    # weights matrix - neuron i's weights are in W[i, :] with bias as the first entry.\n",
    "    def forward_one_layer(self, layeridx : int, input : np.array):\n",
    "        f, df = get_act_by_name(self.hlayer_types[layeridx])\n",
    "        input = np.concatenate(([1], input)) # for including bias\n",
    "        act_values = np.matmul(self.weights[layeridx], input)\n",
    "        output = np.array([f(a) for a in act_values])\n",
    "        self.act_valuesderivs[layeridx] = np.array([df(a) for a in act_values]) # overwrite the act value derivates for this layer\n",
    "        return output\n",
    "\n",
    "    def safe_softmax(self, input : np.array):\n",
    "        prob = np.copy(input)\n",
    "        prob -= np.max(prob)\n",
    "        prob = np.exp(prob) / (np.sum(prob))\n",
    "        return prob\n",
    "\n",
    "    # we store weight matrix, bias vector, activation function of each layer in a dict\n",
    "    def forward(self, input : np.array):\n",
    "        output = np.copy(input)\n",
    "        self.layer_outputs[-1] = input\n",
    "        for idx in range(self.no_of_hlayers + 1):\n",
    "            output = self.forward_one_layer(idx, output)\n",
    "            self.layer_outputs[idx] = np.copy(output)\n",
    "        self.posterior_prob = self.safe_softmax(output)\n",
    "\n",
    "    # backward prop on one **HIDDEN** layer\n",
    "    def backward_one_layer(self, layer_idx):\n",
    "        nl_weighted_deriv_sum = np.matmul((self.weights[layer_idx+1].T)[1:], self.localgradients[layer_idx+1])\n",
    "        self.localgradients[layer_idx] = np.multiply(self.act_valuesderivs[layer_idx], nl_weighted_deriv_sum)\n",
    "        prev_layer_output = np.concatenate(([1], self.layer_outputs[layer_idx - 1]))\n",
    "        self.weight_changes[layer_idx] = self.eta * (np.outer(prev_layer_output, self.localgradients[layer_idx]).T)\n",
    "    \n",
    "    def compute_local_gradients_final_layer(self, target_label, posterior_prob):\n",
    "        output = np.copy(posterior_prob)\n",
    "        output = -output\n",
    "        output[target_label] += 1.0\n",
    "        return output\n",
    "\n",
    "    def backward(self, target_label):\n",
    "        # for output layer\n",
    "        self.localgradients[self.no_of_hlayers] = self.compute_local_gradients_final_layer(target_label, self.posterior_prob)\n",
    "        prev_layer_output = np.concatenate(([1], self.layer_outputs[self.no_of_hlayers - 1]))\n",
    "        self.weight_changes[self.no_of_hlayers] = self.eta * (np.outer(prev_layer_output, self.localgradients[self.no_of_hlayers]).T)\n",
    "        for idx in range(self.no_of_hlayers-1,-1,-1):\n",
    "            self.backward_one_layer(idx)\n",
    "\n",
    "        for idx in range(self.no_of_hlayers+1):\n",
    "            self.weights[idx] += self.weight_changes[idx]\n",
    "    \n",
    "    def train(self, train_X, train_Y, epoches : int):\n",
    "        for i in tqdm(range(epoches)):\n",
    "            print(i)\n",
    "            for (x, y) in zip(train_X, train_Y):\n",
    "                self.forward(x)\n",
    "                self.backward(y)\n",
    "\n",
    "    def validate(self, test_X, test_Y):\n",
    "        correct, sz = 0, test_Y.shape[0]\n",
    "        loss = 0.0\n",
    "        for i in range(sz):\n",
    "            self.forward(test_X[i])\n",
    "            pred = np.argmax(self.posterior_prob)\n",
    "            if pred == test_Y[i]:\n",
    "                correct += 1\n",
    "            loss += (1.0/(i+1)) * (-np.log(self.posterior_prob[test_Y[i]]) - loss)\n",
    "        print(f'accuracy = {correct/sz}; loss = {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "train_count, test_count = X_train.shape[0], X_test.shape[0]\n",
    "X_train = X_train.reshape((train_count, 784))\n",
    "X_test = X_test.reshape((test_count, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 32, 1: 32, 2: 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28899/1406031044.py:38: RuntimeWarning: overflow encountered in matmul\n",
      "  act_values = np.matmul(self.weights[layeridx], input)\n",
      "/tmp/ipykernel_28899/1406031044.py:45: RuntimeWarning: invalid value encountered in subtract\n",
      "  prob -= np.max(prob)\n",
      "  0%|          | 1/1000 [00:42<11:49:40, 42.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:56<15:42:09, 56.59s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m parameters[\u001b[39m'\u001b[39m\u001b[39mNO_OF_CLASSES\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      9\u001b[0m nn \u001b[39m=\u001b[39m ClassificationNeuralNetwork(parameters)\n\u001b[0;32m---> 10\u001b[0m nn\u001b[39m.\u001b[39;49mtrain(X_train, y_train, \u001b[39m1000\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[5], line 86\u001b[0m, in \u001b[0;36mClassificationNeuralNetwork.train\u001b[0;34m(self, train_X, train_Y, epoches)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[39mprint\u001b[39m(i)\n\u001b[1;32m     85\u001b[0m \u001b[39mfor\u001b[39;00m (x, y) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(train_X, train_Y):\n\u001b[0;32m---> 86\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(x)\n\u001b[1;32m     87\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackward(y)\n",
      "Cell \u001b[0;32mIn[5], line 54\u001b[0m, in \u001b[0;36mClassificationNeuralNetwork.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mno_of_hlayers \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 54\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_one_layer(idx, output)\n\u001b[1;32m     55\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_outputs[idx] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mcopy(output)\n\u001b[1;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposterior_prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msafe_softmax(output)\n",
      "Cell \u001b[0;32mIn[5], line 38\u001b[0m, in \u001b[0;36mClassificationNeuralNetwork.forward_one_layer\u001b[0;34m(self, layeridx, input)\u001b[0m\n\u001b[1;32m     36\u001b[0m f, df \u001b[39m=\u001b[39m get_act_by_name(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhlayer_types[layeridx])\n\u001b[1;32m     37\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(([\u001b[39m1\u001b[39m], \u001b[39minput\u001b[39m)) \u001b[39m# for including bias\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m act_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mmatmul(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights[layeridx], \u001b[39minput\u001b[39;49m)\n\u001b[1;32m     39\u001b[0m output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([f(a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m act_values])\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mact_valuesderivs[layeridx] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([df(a) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m act_values]) \u001b[39m# overwrite the act value derivates for this layer\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parameters = dict()\n",
    "parameters['NUMBER_OF_HIDDEN_LAYERS'] = 2\n",
    "parameters['HIDDEN_LAYER_SIZES'] = {0 : 32, 1 : 32}\n",
    "parameters['HIDDEN_LAYER_ACTIVATIONS'] = {0 : 'tanh', 1 : 'tanh'}\n",
    "parameters['OUTPUT_LAYER_ACTIVATION']  = 'tanh'\n",
    "parameters['INPUT_DIMENSION'] = 784\n",
    "parameters['LEARNING_RATE'] = 1e-5\n",
    "parameters['NO_OF_CLASSES'] = 10\n",
    "nn = ClassificationNeuralNetwork(parameters)\n",
    "nn.train(X_train, y_train, 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f695f66d98509fc8b85c7d66cf2105372ef1260fe6dff1d321da4ecb2534873c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
